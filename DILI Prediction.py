# -*- coding: utf-8 -*-
"""Feature_selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G5l_AMphTfDta8wMJBEwPThJ8loQYKSd

https://www.blog.trainindata.com/feature-selection-machine-learning-with-python/#:~:text=In%20feature%20selection%2C%20we%20select,a%20smaller%20set%20of%20features.
"""

# binary classification
import pandas as pd
import seaborn as sns
sns.set(rc={'figure.figsize':(8,8)})
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.metrics import confusion_matrix, accuracy_score

dataset = pd.read_csv('output_v3.csv')
dataset.head(10)

X = dataset.iloc[:, 1:].values
y = dataset.iloc[:, 0].values
# Scale the input features
#scaler = StandardScaler()
#X = scaler.fit_transform(X)
"""Plot the histogram of the terget value"""

sns.histplot(y)

from sklearn.feature_selection import VarianceThreshold
# To remove constant features
sel = VarianceThreshold(threshold=0)

# fit finds the features with zero variance
X_t = sel.fit_transform(X)
# Separate data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

scaler = StandardScaler()
scaler.fit(X_train)

"""Select features utilizing logistic regression as a classifier, with the Lasso regularization:"""

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import Lasso, LogisticRegression
selector = SelectFromModel(
    LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=10))

selector.fit(scaler.transform(X_train), y_train)

selector.get_support()

X_train_selected = selector.transform(scaler.transform(X_train))
X_test_selected = selector.transform(scaler.transform(X_test))

X_train_selected.shape

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
# Create the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the training data
rf_classifier.fit(X_train_selected, y_train)

# Make predictions on the test data
y_pred = rf_classifier.predict(X_test_selected)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Random Forest Classifier: {accuracy:.2f}")

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold

# Define the number of repeats and folds
n_repeats = 100
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    rf_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = rf_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, rf_classifier.predict_proba(X_val_fold)[:, 1]))

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

mean_auc = np.mean(auc_scores)
std_auc = np.std(auc_scores)

# Print the results
print(f"Mean Accuracy: {mean_accuracy:.4f} (Std: {std_accuracy:.4f})")
print(f"Mean Precision: {mean_precision:.4f} (Std: {std_precision:.4f})")
print(f"Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})")
print(f"Mean MCC: {mean_mcc:.4f} (Std: {std_mcc:.4f})")
print(f"Mean F1 Score: {mean_f1:.4f} (Std: {std_f1:.4f})")
print(f"Mean AUC: {mean_auc:.4f} (Std: {std_auc:.4f})")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import RocCurveDisplay, auc

# Define the number of repeats and folds
n_repeats = 1
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Initialize variables for ROC curve plotting
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

# Create a figure for ROC curve
fig, ax = plt.subplots(figsize=(6, 6))

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    rf_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = rf_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, rf_classifier.predict_proba(X_val_fold)[:, 1]))

    # Calculate ROC curve for the current fold
    viz = RocCurveDisplay.from_estimator(
        rf_classifier,
        X_val_fold,
        y_val_fold,
        name=f"ROC fold {len(aucs)}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

# Plot the mean ROC curve
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

# Set plot labels and title
ax.set(
    xlim=[-0.05, 1.05],
    ylim=[-0.05, 1.05],
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title="Mean ROC curve for RF",
)
ax.axis("square")
ax.legend(loc="lower right")
plt.show()
#title="Mean ROC curve with variability "

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold

# Define the number of repeats and folds
n_repeats = 100
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the Logistic Regression classifier
logistic_regression = LogisticRegression(random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    logistic_regression.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = logistic_regression.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, logistic_regression.predict_proba(X_val_fold)[:, 1]))

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

mean_auc = np.mean(auc_scores)
std_auc = np.std(auc_scores)

# Print the results
print(f"Mean Accuracy: {mean_accuracy:.4f} (Std: {std_accuracy:.4f})")
print(f"Mean Precision: {mean_precision:.4f} (Std: {std_precision:.4f})")
print(f"Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})")
print(f"Mean MCC: {mean_mcc:.4f} (Std: {std_mcc:.4f})")
print(f"Mean F1 Score: {mean_f1:.4f} (Std: {std_f1:.4f})")
print(f"Mean AUC: {mean_auc:.4f} (Std: {std_auc:.4f})")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import RocCurveDisplay, auc

# Define the number of repeats and folds
n_repeats = 1
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the Logistic Regression classifier
logistic_regression = LogisticRegression(random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Initialize variables for ROC curve plotting
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

# Create a figure for ROC curve
fig, ax = plt.subplots(figsize=(6, 6))

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the logistic regression model on the training data
    logistic_regression.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = logistic_regression.predict(X_val_fold)
    y_prob = logistic_regression.predict_proba(X_val_fold)[:, 1]

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, y_prob))

    # Calculate ROC curve for the current fold
    viz = RocCurveDisplay.from_estimator(
        logistic_regression,
        X_val_fold,
        y_val_fold,
        name=f"ROC fold {len(aucs)}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

# Plot the mean ROC curve
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

# Set plot labels and title
ax.set(
    xlim=[-0.05, 1.05],
    ylim=[-0.05, 1.05],
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title="Mean ROC curve for Logistic Regression",
)
ax.axis("square")
ax.legend(loc="lower right")
plt.show()

import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold

# Define the number of repeats and folds
n_repeats = 100
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the SVM classifier
svm_classifier = SVC(kernel='linear', probability=True, random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    svm_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = svm_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, svm_classifier.predict_proba(X_val_fold)[:, 1]))

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

mean_auc = np.mean(auc_scores)
std_auc = np.std(auc_scores)

# Print the results
print(f"Mean Accuracy: {mean_accuracy:.4f} (Std: {std_accuracy:.4f})")
print(f"Mean Precision: {mean_precision:.4f} (Std: {std_precision:.4f})")
print(f"Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})")
print(f"Mean MCC: {mean_mcc:.4f} (Std: {std_mcc:.4f})")
print(f"Mean F1 Score: {mean_f1:.4f} (Std: {std_f1:.4f})")
print(f"Mean AUC: {mean_auc:.4f} (Std: {std_auc:.4f})")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import RocCurveDisplay, auc

# Define the number of repeats and folds
n_repeats = 1
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the SVM classifier
svm_classifier = SVC(probability=True, random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Initialize variables for ROC curve plotting
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

# Create a figure for ROC curve
fig, ax = plt.subplots(figsize=(6, 6))

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the SVM classifier on the training data
    svm_classifier.fit(X_train_fold, y_train_fold)

    # Predict probabilities on the validation set
    y_pred_prob = svm_classifier.predict_proba(X_val_fold)[:, 1]

    # Predict binary labels based on a threshold (e.g., 0.5)
    y_pred = (y_pred_prob > 0.5).astype(int)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, y_pred_prob))

    # Calculate ROC curve for the current fold
    viz = RocCurveDisplay.from_estimator(
        svm_classifier,
        X_val_fold,
        y_val_fold,
        name=f"ROC fold {len(aucs)}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

# Plot the mean ROC curve
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

# Set plot labels and title
ax.set(
    xlim=[-0.05, 1.05],
    ylim=[-0.05, 1.05],
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title="Mean ROC curve for SVM",
)
ax.axis("square")
ax.legend(loc="lower right")
plt.show()

import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold

# Define the number of repeats and folds
n_repeats = 100
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    dt_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = dt_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, dt_classifier.predict_proba(X_val_fold)[:, 1]))

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

mean_auc = np.mean(auc_scores)
std_auc = np.std(auc_scores)

# Print the results
print(f"Mean Accuracy: {mean_accuracy:.4f} (Std: {std_accuracy:.4f})")
print(f"Mean Precision: {mean_precision:.4f} (Std: {std_precision:.4f})")
print(f"Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})")
print(f"Mean MCC: {mean_mcc:.4f} (Std: {std_mcc:.4f})")
print(f"Mean F1 Score: {mean_f1:.4f} (Std: {std_f1:.4f})")
print(f"Mean AUC: {mean_auc:.4f} (Std: {std_auc:.4f})")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import RocCurveDisplay, auc

# Define the number of repeats and folds
n_repeats = 1
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Initialize variables for ROC curve plotting
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

# Create a figure for ROC curve
fig, ax = plt.subplots(figsize=(6, 6))

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    dt_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = dt_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, dt_classifier.predict_proba(X_val_fold)[:, 1]))

    # Calculate ROC curve for the current fold
    viz = RocCurveDisplay.from_estimator(
        dt_classifier,
        X_val_fold,
        y_val_fold,
        name=f"ROC fold {len(aucs)}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

# Plot the mean ROC curve
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

# Set plot labels and title
ax.set(
    xlim=[-0.05, 1.05],
    ylim=[-0.05, 1.05],
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title="Mean ROC curve for Decision Tree Classifier",
)
ax.axis("square")
ax.legend(loc="lower right")
plt.show()

import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold

# Define the number of repeats and folds
n_repeats = 100
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the k-Nearest Neighbors classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    knn_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = knn_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, knn_classifier.predict_proba(X_val_fold)[:, 1]))

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

mean_auc = np.mean(auc_scores)
std_auc = np.std(auc_scores)

# Print the results
print(f"Mean Accuracy: {mean_accuracy:.4f} (Std: {std_accuracy:.4f})")
print(f"Mean Precision: {mean_precision:.4f} (Std: {std_precision:.4f})")
print(f"Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})")
print(f"Mean MCC: {mean_mcc:.4f} (Std: {std_mcc:.4f})")
print(f"Mean F1 Score: {mean_f1:.4f} (Std: {std_f1:.4f})")
print(f"Mean AUC: {mean_auc:.4f} (Std: {std_auc:.4f})")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import RocCurveDisplay, auc

# Define the number of repeats and folds
n_repeats = 1
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the K-NN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Initialize variables for ROC curve plotting
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

# Create a figure for ROC curve
fig, ax = plt.subplots(figsize=(6, 6))

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the K-NN classifier on the training data
    knn_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = knn_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, knn_classifier.predict_proba(X_val_fold)[:, 1]))

    # Calculate ROC curve for the current fold
    viz = RocCurveDisplay.from_estimator(
        knn_classifier,
        X_val_fold,
        y_val_fold,
        name=f"ROC fold {len(aucs)}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

# Plot the mean ROC curve
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

# Set plot labels and title
ax.set(
    xlim=[-0.05, 1.05],
    ylim=[-0.05, 1.05],
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title="Mean ROC curve for KNN",
)
ax.axis("square")
ax.legend(loc="lower right")
plt.show()

import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold

# Define the number of repeats and folds
n_repeats = 100
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the Naive Bayes classifier
nb_classifier = GaussianNB()

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    nb_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = nb_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, nb_classifier.predict_proba(X_val_fold)[:, 1]))

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

mean_auc = np.mean(auc_scores)
std_auc = np.std(auc_scores)

# Print the results
print(f"Mean Accuracy: {mean_accuracy:.4f} (Std: {std_accuracy:.4f})")
print(f"Mean Precision: {mean_precision:.4f} (Std: {std_precision:.4f})")
print(f"Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})")
print(f"Mean MCC: {mean_mcc:.4f} (Std: {std_mcc:.4f})")
print(f"Mean F1 Score: {mean_f1:.4f} (Std: {std_f1:.4f})")
print(f"Mean AUC: {mean_auc:.4f} (Std: {std_auc:.4f})")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import RocCurveDisplay, auc

# Define the number of repeats and folds
n_repeats = 1
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the Naive Bayes classifier
nb_classifier = GaussianNB()

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Initialize variables for ROC curve plotting
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

# Create a figure for ROC curve
fig, ax = plt.subplots(figsize=(6, 6))

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    nb_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = nb_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, nb_classifier.predict_proba(X_val_fold)[:, 1]))

    # Calculate ROC curve for the current fold
    viz = RocCurveDisplay.from_estimator(
        nb_classifier,
        X_val_fold,
        y_val_fold,
        name=f"ROC fold {len(aucs)}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

# Plot the mean ROC curve
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

# Set plot labels and title
ax.set(
    xlim=[-0.05, 1.05],
    ylim=[-0.05, 1.05],
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title="Mean ROC curve with variability",
)
ax.axis("square")
ax.legend(loc="lower right")
plt.show()

import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold

# Define the number of repeats and folds
n_repeats = 100
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Define the number of neighbors (you can adjust this)
n_neighbors = 5

# Create the k-NN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=n_neighbors)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    knn_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = knn_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, knn_classifier.predict_proba(X_val_fold)[:, 1]))

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

mean_auc = np.mean(auc_scores)
std_auc = np.std(auc_scores)

# Print the results
print(f"Mean Accuracy: {mean_accuracy:.4f} (Std: {std_accuracy:.4f})")
print(f"Mean Precision: {mean_precision:.4f} (Std: {std_precision:.4f})")
print(f"Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})")
print(f"Mean MCC: {mean_mcc:.4f} (Std: {std_mcc:.4f})")
print(f"Mean F1 Score: {mean_f1:.4f} (Std: {std_f1:.4f})")
print(f"Mean AUC: {mean_auc:.4f} (Std: {std_auc:.4f})")

X_train_selected.shape

X_test_selected.shape

import numpy as np
import pandas as pd
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold

# Define the number of repeats and folds
n_repeats = 100
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the MLP classifier
mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the classifier on the training data
    mlp_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = mlp_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, mlp_classifier.predict_proba(X_val_fold)[:, 1]))

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

mean_auc = np.mean(auc_scores)
std_auc = np.std(auc_scores)

# Print the results
print(f"Mean Accuracy: {mean_accuracy:.4f} (Std: {std_accuracy:.4f})")
print(f"Mean Precision: {mean_precision:.4f} (Std: {std_precision:.4f})")
print(f"Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})")
print(f"Mean MCC: {mean_mcc:.4f} (Std: {std_mcc:.4f})")
print(f"Mean F1 Score: {mean_f1:.4f} (Std: {std_f1:.4f})")
print(f"Mean AUC: {mean_auc:.4f} (Std: {std_auc:.4f})")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import RocCurveDisplay, auc

# Define the number of repeats and folds
n_repeats = 1
n_folds = 10

# Initialize lists to store evaluation metrics for each repeat
accuracy_scores = []
precision_scores = []
recall_scores = []
mcc_scores = []
f1_scores = []
auc_scores = []

# Create the MLP classifier
mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)

# Create the RepeatedKFold cross-validator
rkf = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)

# Initialize variables for ROC curve plotting
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

# Create a figure for ROC curve
fig, ax = plt.subplots(figsize=(6, 6))

# Perform cross-validation and evaluation
for train_index, val_index in rkf.split(X_train_selected):
    X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the MLP classifier on the training data
    mlp_classifier.fit(X_train_fold, y_train_fold)

    # Predict on the validation set
    y_pred = mlp_classifier.predict(X_val_fold)

    # Calculate evaluation metrics
    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))
    precision_scores.append(precision_score(y_val_fold, y_pred))
    recall_scores.append(recall_score(y_val_fold, y_pred))
    mcc_scores.append(matthews_corrcoef(y_val_fold, y_pred))
    f1_scores.append(f1_score(y_val_fold, y_pred))
    auc_scores.append(roc_auc_score(y_val_fold, mlp_classifier.predict_proba(X_val_fold)[:, 1]))

    # Calculate ROC curve for the current fold
    viz = RocCurveDisplay.from_estimator(
        mlp_classifier,
        X_val_fold,
        y_val_fold,
        name=f"ROC fold {len(aucs)}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

# Calculate mean and standard deviation of evaluation metrics
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)

mean_precision = np.mean(precision_scores)
std_precision = np.std(precision_scores)

mean_recall = np.mean(recall_scores)
std_recall = np.std(recall_scores)

mean_mcc = np.mean(mcc_scores)
std_mcc = np.std(mcc_scores)

mean_f1 = np.mean(f1_scores)
std_f1 = np.std(f1_scores)

# Plot the mean ROC curve
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

# Set plot labels and title
ax.set(
    xlim=[-0.05, 1.05],
    ylim=[-0.05, 1.05],
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title="Mean ROC curve for MLP",
)
ax.axis("square")
ax.legend(loc="lower right")
plt.show()